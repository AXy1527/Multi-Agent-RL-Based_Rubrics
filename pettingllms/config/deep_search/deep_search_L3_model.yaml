defaults:
  - ../ppo_trainer@models.model_0. ppo_trainer_config: eval
  - ../ppo_trainer@models.model_1.ppo_trainer_config: eval
  - ../ppo_trainer@models.model_2.ppo_trainer_config: eval
  - _self_

specialization: "full"  # L3: each agent uses different model

resource:
  nnodes: 1
  n_gpus_per_node: 8
  trust_remote_code: true

# Environment configuration for Deep Search
env:
  name: deep_search_env
  dataset: "bamboogle"
  benchmark: "bamboogle"
  max_turns: 10
  max_rounds: 10
  max_docs_per_query: 5
  resolve: false
  multi_modal: false
  batched_init: true

# Base models for L3 (3 different models for 3 agents)
base_models:
  policy_0:
    path: "your_q_agent_model_path"
    name: "q_agent_model"
  policy_1:
    path: "your_r_agent_model_path"
    name: "r_agent_model"
  policy_2:
    path: "your_a_agent_model_path"
    name: "a_agent_model"

# Multi-agent configuration for Deep Search (Q, R, A agents)
agent_policy_configs:
  num_agents: 3
  policy_list: ["q_agent", "r_agent", "a_agent"]
  agent_configs:
    agent_0:
      name: "q_agent"
      policy_name: "q_agent_model"
      # Q-Agent specific configs
#      train_temperature: 0.8
#      val_temperature: 0.6

    agent_1:
      name: "r_agent"
      policy_name: "r_agent_model"
      # R-Agent specific configs
#      train_temperature: 0.7
#      val_temperature: 0.5

    agent_2:
      name: "a_agent"
      policy_name: "a_agent_model"
      # A-Agent specific configs
#      train_temperature: 0.7
#      val_temperature: 0.5

# Multi-agent interaction configuration
multi_agent_interaction:
  # Turn order follows the Q -> R -> Q -> R -> ...  -> A pattern
  # This is dynamic based on environment state, not fixed
  turn_order: ["q_agent", "r_agent", "a_agent"]
  num_interacting_agents: 3

  # Deep Search specific: RM-Agent configuration (not trained)
  rm_agent:
    enabled: true
    api_url: ${oc.env:RM_LLM_API_URL,https://api.openai.com/v1/chat/completions}
    api_key: ${oc.env:RM_LLM_API_KEY,}
    model: ${oc.env:RM_LLM_MODEL,gpt-4}

training:
  device: cuda
  total_training_steps: 200
  project_name: pettingllms
  experiment_name: deep_search_L3_model
  logger: ['console', 'wandb']
  model_checkpoints_dir: checkpoints
  ray_wait_register_center_timeout: 300
  train_batch_size: 32
  train_sample_num: 8
  validate_sample_num: 1
  sample_temperature: 1
  val_freq: 10
  resample_freq: 3
  epoch_size: 20
  max_prompt_length: 8192
  max_response_length: 16384

# Models configuration for L3 (3 separate models)
models:
  model_0:
    path: ${base_models.policy_0.path}
    name: ${base_models.policy_0.name}
    ppo_trainer_config:
      filter_method: mean
      filter_ratio: 0.5
      data:
        max_prompt_length: ${training.max_prompt_length}
        max_response_length: ${training.max_response_length}
      actor_rollout_ref:
        model:
          path: ${base_models.policy_0.path}
        rollout:
          temperature: ${training.sample_temperature}
          prompt_length: ${training.max_prompt_length}
          response_length: ${training.max_response_length}
          tensor_model_parallel_size: ${resource.n_gpus_per_node}
        trainer:
          n_gpus_per_node: ${resource.n_gpus_per_node}
          n_training_gpus_per_node: ${resource.n_gpus_per_node}

  model_1:
    path: ${base_models.policy_1.path}
    name: ${base_models.policy_1.name}
    ppo_trainer_config:
      filter_method: mean
      filter_ratio: 0.3
      data:
        max_prompt_length: ${training.max_prompt_length}
        max_response_length: ${training.max_response_length}
      actor_rollout_ref:
        model:
          path: ${base_models.policy_1.path}
        rollout:
          temperature: ${training.sample_temperature}
          prompt_length: ${training.max_prompt_length}
          response_length: ${training.max_response_length}
          tensor_model_parallel_size: ${resource.n_gpus_per_node}
        trainer:
          n_gpus_per_node: ${resource.n_gpus_per_node}
          n_training_gpus_per_node: ${resource.n_gpus_per_node}

  model_2:
    path: ${base_models.policy_2.path}
    name: ${base_models.policy_2.name}
    ppo_trainer_config:
      filter_method: mean
      filter_ratio: 0.3
      data:
        max_prompt_length: ${training.max_prompt_length}
        max_response_length: ${training.max_response_length}
      actor_rollout_ref:
        model:
          path: ${base_models.policy_2.path}
        rollout:
          temperature: ${training.sample_temperature}
          prompt_length: ${training.max_prompt_length}
          response_length: ${training.max_response_length}
          tensor_model_parallel_size: ${resource.n_gpus_per_node}
        trainer:
          n_gpus_per_node: ${resource.n_gpus_per_node}
          n_training_gpus_per_node: ${resource.n_gpus_per_node}